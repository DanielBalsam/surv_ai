{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.language.gpt import GPTClient\n",
    "from dotenv import load_dotenv\n",
    "from lib.language.interfaces import Prompt, PromptMessage\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ[\"OPEN_AI_API_KEY\"]\n",
    "\n",
    "client = GPTClient(API_KEY, temperature=0.1, model=\"gpt-3.5-turbo-0301\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm sorry, as an AI language model, I do not have access to real-time information and cannot provide you with the current date. Please check your device or search engine for the current date.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = Prompt(\n",
    "    messages=[\n",
    "        PromptMessage(role=\"user\", content=\"What's the date?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "await client.get_completions([prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is April 17th, 2023.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.agent import ResearchAgent\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "agent = ResearchAgent(client)\n",
    "\n",
    "await agent.teach(f\"Today's date is {datetime.now().strftime('%B %dth, %Y')}.\")\n",
    "await agent.teach(f\"Yesterday's date is {(datetime.now() - timedelta(days=1)).strftime('%B %dth, %Y')}.\")\n",
    "await agent.teach(f\"Tomorrow's date is {(datetime.now() + timedelta(days=1)).strftime('%B %dth, %Y')}.\")\n",
    "await agent.teach(f\"The sky is blue.\")\n",
    "await agent.teach(f\"Squirrels are cute\")\n",
    "await agent.teach(f\"Programming is fun\")\n",
    "await agent.teach(f\"Python is a great programming language\")\n",
    "await agent.teach(f\"Pandas is a great Python library\")\n",
    "await agent.teach(f\"Eating healthy is important\")\n",
    "await agent.teach(f\"Pizza is delicious\")\n",
    "\n",
    "await agent.prompt(\"What's the date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the information I found, some controversial areas of research in machine learning include fairness, bias, privacy, and interpretability. Fairness refers to attempts to correct algorithmic bias in decision processes based on machine learning models, while privacy and interpretability are also areas of concern. The problem of algorithmic bias is well-known and well-studied, with research focusing on the origins of bias, types of bias, and methods to reduce bias.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.tools.research.wikipedia import WikipediaTool\n",
    "from core.tools.toolbelt import Toolbelt\n",
    "\n",
    "agent = ResearchAgent(client, toolbelt=Toolbelt(client, tools=[WikipediaTool(client, n_pages=5)]))\n",
    "\n",
    "await agent.prompt(\"what are some controversial area of research in machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fairness in machine learning is controversial because there is no universal definition of fairness, and different definitions can be in contradiction with each other. Additionally, decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive, such as gender, ethnicity, sexual orientation, disability, and more. This can lead to algorithmic bias, which is a well-known and well-studied problem in machine learning.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.prompt(\"why is fairness in machine learning controversial?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bias refers to the presence of systematic errors in a machine learning model that can lead to unfair decisions. Fairness, on the other hand, refers to the absence of discrimination or prejudice in decision-making processes. In other words, bias can lead to unfairness, and fairness can be achieved by reducing bias. However, defining what is fair and what is biased can be controversial and context-dependent.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.prompt(\"what is the difference between bias and fairness?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Margaret Mitchell is a computer scientist who specializes in algorithmic bias and fairness in machine learning. Her work focuses on removing undesired biases from machine learning models related to demographic groups and promoting more transparent reporting of their intended use. She joined Google Research and Machine Intelligence in 2016 as a Senior Research Scientist and co-led the Ethical Artificial Intelligence team with Timnit Gebru.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.prompt(\"who is margaret mitchell?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eliezer Yudkowsky is a self-taught computer scientist who is known for his work on the safety challenges posed by future generations of AI systems. He has proposed that autonomous and adaptive systems be designed to learn correct behavior over time and has written about the risk of artificial intelligence. Yudkowsky has also proposed potential actions that could be taken to limit the risk of AI, including a total halt on its development.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.prompt(\"who is Eliezer Yudkowsky?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I cannot find any information on what Margaret Mitchell and Eliezer Yudkowsky have in common. My previous search results only provided information on Margaret Mitchell's work on ethics, bias, privacy, and interpretability in machine learning. Eliezer Yudkowsky is a researcher in artificial intelligence and decision theory, but there is no clear connection between his work and Mitchell's.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agent.prompt(\"what is one thing Margaret Mitchell have in common with Eliazer Yudkowsky?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superintelligence-kEKUOLhR-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
